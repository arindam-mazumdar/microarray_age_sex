{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microarray data analysis\n",
    "\n",
    "Microarray data in genetics refers to a type of high-throughput technology used to measure the expression levels of thousands of genes simultaneously. Microarrays are composed of small glass slides or silicon chips on which thousands of microscopic spots, known as probes, are deposited in an orderly grid pattern. These probes are short DNA sequences or fragments that are complementary to specific genes or regions of the genome. Each spot represents a particular gene or DNA sequence. The results are typically strored in a .CEL file. To process a .CEL file we go thourgh the following steps\n",
    "\n",
    "- Get .CEL from https://www.ncbi.nlm.nih.gov/\n",
    "- Every .CEL file corresponds to an experiment which is part of SERIES and SERIES is a based on a platform.\n",
    "- We use GPL570 platform to restrict our-selves to **Affymetrix Human Genome U133 Plus 2.0 Array**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting HTML files of sample description\n",
    "\n",
    "- Download the list of all accession of GPL570 at https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GPL570&targ=self&view=brief&form=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all sample_ids into a list:\n",
    "\n",
    "with open('GPL570.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    sample_id = []\n",
    "    for line in lines:\n",
    "        if line[:22] == '!Platform_sample_id = ':\n",
    "            sample_id.append('GSM'+rx.findall(r'\\d+',line)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import regex as rx\n",
    "\n",
    "# Download all html files corresponding to the sample ids.\n",
    "\n",
    "for idx in sample_id:\n",
    "    url = 'https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc='+idx\n",
    "    page = re.get(url)\n",
    "    with open('./html_files/'+idx+'.html', 'wb') as file:\n",
    "        file.write(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Age and Sex information from the HTML files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_sex(nn):\n",
    "    '''\n",
    "    This function simply match patterns in the text where gender is explicitly \n",
    "    defined and age is mentioned as \"Age\". However, we have encountered there  \n",
    "    are  many other types of Sex/Age information like : F/26 and others. We \n",
    "    intentionally did not make it complicated, since processing too many files \n",
    "    in laptop is beyond its capacity. \n",
    "    '''\n",
    "    with open('./html_files/'+sample_id[nn]+'.html','r') as f:\n",
    "        page = f.read()\n",
    "    soup = BeautifulSoup(page,'html.parser')\n",
    "    text = soup.get_text(separator=' ')\n",
    "    loc = text.find(' Age')\n",
    "    if loc > 0:\n",
    "        age = rx.findall(r'\\d+', text[loc:loc+20])\n",
    "    else:\n",
    "        age = np.nan\n",
    "        \n",
    "    sex_list = rx.findall(r'(Male|Female|male|female)\\s+', text)\n",
    "    if len(sex_list) > 0:\n",
    "        sex = sex_list[0]\n",
    "    else:\n",
    "        sex = np.nan\n",
    "    return age, sex\n",
    "\n",
    "\n",
    "## Parallelised extraction of age/asex information\n",
    "\n",
    "pool = Pool() ### number of processesor I want to use\n",
    "out = zip(*pool.map(get_age_sex, range(169340)))\n",
    "age_sex_list = list(out)\n",
    "\n",
    "\n",
    "# Arrange ages with Minimum(age) and Maximum(Age) information \n",
    "\n",
    "age_list = age_sex_list[0]\n",
    "sex_list = age_sex_list[1]\n",
    "\n",
    "age_list_1 = []\n",
    "age_list_2 = []\n",
    "\n",
    "for ages in age_list:\n",
    "    if type(ages) is not list:\n",
    "        age1 = ages\n",
    "        age2 = ages\n",
    "    elif len(ages) > 1:\n",
    "        age1 = ages[0]\n",
    "        age2 = ages[1]\n",
    "    elif len(ages) == 1:\n",
    "        age1 = ages[0]\n",
    "        age2 = ages[0]\n",
    "    else:\n",
    "        age1 = np.nan\n",
    "        age2 = np.nan\n",
    "    age_list_1.append(age1)\n",
    "    age_list_2.append(age2)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'sample_id' : sample_id, 'age_1': age_list_1, 'age_2':age_list_2 , 'sex': sex_list})\n",
    "\n",
    "## Write the result in a file\n",
    "df_filtered = df.dropna()\n",
    "df_filtered.to_csv('age_sex_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the .CEL files\n",
    "\n",
    "We download the .CEL files corresponding to the sample_ids whose age/sex infromation is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a txt file which contains all the downlaod links\n",
    "\n",
    "for idx in df_filtered.sample_id:\n",
    "    with open('./html_files/'+idx+'.html','r') as f:\n",
    "        page = f.read()\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href[:3] == 'ftp':\n",
    "            with open('./cel_files/cel_links.txt','a') as ff:\n",
    "                ff.write(href+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the cell files using bash script\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "count=0  # Initialize a variable to keep track of the number of files downloaded\n",
    "\n",
    "# Read the file line by line\n",
    "while IFS= read -r link\n",
    "do\n",
    "    # Use wget to download each file\n",
    "    wget \"$link\"\n",
    "    ((count++))  # Increment the count after each successful download\n",
    "done < cel_links.txt\n",
    "\n",
    "# Output the number of files downloaded as a comment\n",
    "echo \"Number of files downloaded: $count\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fRMA Normaization of .CEL files\n",
    "\n",
    "Frozen RMA (fRMA) is a microarray preprocessing algorithm that allows one to analyze\n",
    "microarrays individually or in small batches and then combine the data for analysis. This\n",
    "is accomplished by utilizing information from the large publicly available microarray\n",
    "databases. Specifically, estimates of probe-specific effects and variances are precomputed\n",
    "and frozen. Then, with new data sets, these frozen parameters are used in concert with\n",
    "information from the new array(s) to preprocess the data.\n",
    "Follow documentation: https://www.bioconductor.org/packages/release/bioc/html/frma.html\n",
    "\n",
    "The following **R-script** does the fRMA transformation and saves output of z-scores in .csv format. \n",
    "\n",
    "```R\n",
    "library(frma)\n",
    "library(affy)\n",
    "\n",
    "\n",
    "input_path = paste0('.../cel_files/',i,'.CEL.gz')\n",
    "Data <- ReadAffy(filenames = input_path)\n",
    "\n",
    "# for custom CDF file use:\n",
    "# Data <- ReadAffy(filenames = input_path, cdfname = '...path_to/GPL17996_HGU133Plus2_Hs_ENTREZG.cdf')\n",
    "\n",
    "\n",
    "Object <- frma(Data)\n",
    "   bc<- barcode(Object, output = 'z-score')\n",
    "   output_path = paste0('..../expr_files/',i,'.csv')\n",
    "   write.csv(bc, file = output_path)}\n",
    "```\n",
    "\n",
    "For processing all the .cel files in a parallelized for loop look at **get_z_score_parallel.r**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
